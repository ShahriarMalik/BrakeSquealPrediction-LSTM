{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "189bb664",
   "metadata": {},
   "source": [
    "# Predicting Brake Squealing Events Using LSTM Models: Analyzing the Impact of 'Trigger2' as Brake Pad and Brake Disc Contact Indicator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "386dbc8b",
   "metadata": {},
   "source": [
    "## Predicting isSquealing with filtered Trigger2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6abb6687",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39285555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path containing the CSV files\n",
    "directory = \"Two_Dataset_Selected_Columns_Int\"\n",
    "\n",
    "# Select the specific file for testing\n",
    "file_to_test = \"essai99_derivativeDataIA_combined.csv\"\n",
    "\n",
    "# Lists to store data\n",
    "X_train_data = []\n",
    "y_train_data = []\n",
    "X_val_data = []\n",
    "y_val_data = []\n",
    "X_test_data = []\n",
    "y_test_data = []\n",
    "\n",
    "# Split the files into training, validation, and testing sets\n",
    "file_list = os.listdir(directory)\n",
    "train_files, val_files, test_files = np.split(file_list, [int(len(file_list) * 0.7), int(len(file_list) * 0.85)])\n",
    "\n",
    "# Remove the test file from the test_files list\n",
    "test_files = [file for file in test_files if file != file_to_test]\n",
    "\n",
    "# Iterate over files in the specified directory\n",
    "for file in file_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, file)\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "        # Filter out instances where Trigger2 is not close to the most occurred value\n",
    "        trigger2_mode = data['Trigger2'].mode()[0]\n",
    "        close_threshold = 0.01  # Adjust this threshold as per your requirement\n",
    "        data_filtered = data[np.abs(data['Trigger2'] - trigger2_mode) < close_threshold]\n",
    "#         data_filtered = data\n",
    "        # Find the index of the 'isSquealing' column\n",
    "        isSquealing_index = list(data.columns).index('isSquealing')\n",
    "\n",
    "        # Split the data into input sequences (X) and output sequences (y)\n",
    "        window_size = 6\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(data_filtered) - window_size):\n",
    "            X.append(data_filtered.iloc[i:i + window_size, :-1].values)\n",
    "            y.append(data_filtered.iloc[i + window_size, isSquealing_index])\n",
    "\n",
    "        if file in train_files:\n",
    "            X_train_data.extend(X)\n",
    "            y_train_data.extend(y)\n",
    "        elif file in val_files:\n",
    "            X_val_data.extend(X)\n",
    "            y_val_data.extend(y)\n",
    "        elif file == file_to_test:\n",
    "            X_test_data.extend(X)\n",
    "            y_test_data.extend(y)\n",
    "\n",
    "# Convert the lists to arrays\n",
    "X_train_data = np.array(X_train_data)\n",
    "y_train_data = np.array(y_train_data)\n",
    "X_val_data = np.array(X_val_data)\n",
    "y_val_data = np.array(y_val_data)\n",
    "X_test_data = np.array(X_test_data)\n",
    "y_test_data = np.array(y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f044c5f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Reshape the input data to 2-dimensional shape\n",
    "X_train_flat = X_train_data.reshape((X_train_data.shape[0], -1))\n",
    "X_val_flat = X_val_data.reshape((X_val_data.shape[0], -1))\n",
    "X_test_flat = X_test_data.reshape((X_test_data.shape[0], -1))\n",
    "\n",
    "# Handle missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_flat = imputer.fit_transform(X_train_flat)\n",
    "X_val_flat = imputer.transform(X_val_flat)\n",
    "X_test_flat = imputer.transform(X_test_flat)\n",
    "\n",
    "# Reshape the data back to the original shape\n",
    "X_train = X_train_flat.reshape((X_train_data.shape[0], window_size, -1))\n",
    "X_val = X_val_flat.reshape((X_val_data.shape[0], window_size, -1))\n",
    "X_test = X_test_flat.reshape((X_test_data.shape[0], window_size, -1))\n",
    "\n",
    "# Reshape the data for StandardScaler\n",
    "X_train_scaled = X_train.reshape((X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "X_val_scaled = X_val.reshape((X_val.shape[0] * X_val.shape[1], X_val.shape[2]))\n",
    "X_test_scaled = X_test.reshape((X_test.shape[0] * X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Apply StandardScaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "X_val_scaled = scaler.transform(X_val_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_scaled)\n",
    "\n",
    "# Reshape X_train_scaled, X_val_scaled, and X_test_scaled back to the original shape\n",
    "X_train_scaled = X_train_scaled.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_val_scaled = X_val_scaled.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2]))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231ceb5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5575d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path containing the CSV files\n",
    "directory = \"Two_Dataset_Selected_Columns_Int\"\n",
    "\n",
    "# Select the specific file for testing\n",
    "file_to_test = \"essai99_derivativeDataIA_combined.csv\"\n",
    "\n",
    "# Lists to store data\n",
    "X_train_data = []\n",
    "y_train_data = []\n",
    "X_val_data = []\n",
    "y_val_data = []\n",
    "X_test_data = []\n",
    "y_test_data = []\n",
    "\n",
    "# Split the files into training, validation, and testing sets\n",
    "file_list = os.listdir(directory)\n",
    "train_files, val_files, test_files = np.split(file_list, [int(len(file_list) * 0.7), int(len(file_list) * 0.85)])\n",
    "\n",
    "# Remove the test file from the test_files list\n",
    "test_files = [file for file in test_files if file != file_to_test]\n",
    "\n",
    "# Iterate over files in the specified directory\n",
    "for file in file_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, file)\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "        # Filter out instances where Trigger2 is not close to the most occurred value\n",
    "        trigger2_mode = data['Trigger2'].mode()[0]\n",
    "        close_threshold = 0.01  # Adjust this threshold as per your requirement\n",
    "        data_filtered = data[np.abs(data['Trigger2'] - trigger2_mode) < close_threshold]\n",
    "#         data_filtered = data\n",
    "        # Find the index of the 'isSquealing' column\n",
    "        isSquealing_index = list(data.columns).index('isSquealing')\n",
    "\n",
    "        # Split the data into input sequences (X) and output sequences (y)\n",
    "        window_size = 6\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(data_filtered) - window_size):\n",
    "            X.append(data_filtered.iloc[i:i + window_size, :-1].values)\n",
    "            y.append(data_filtered.iloc[i + window_size, isSquealing_index])\n",
    "\n",
    "        if file in train_files:\n",
    "            X_train_data.extend(X)\n",
    "            y_train_data.extend(y)\n",
    "        elif file in val_files:\n",
    "            X_val_data.extend(X)\n",
    "            y_val_data.extend(y)\n",
    "        elif file == file_to_test:\n",
    "            X_test_data.extend(X)\n",
    "            y_test_data.extend(y)\n",
    "\n",
    "# Convert the lists to arrays\n",
    "X_train_data = np.array(X_train_data)\n",
    "y_train_data = np.array(y_train_data)\n",
    "X_val_data = np.array(X_val_data)\n",
    "y_val_data = np.array(y_val_data)\n",
    "X_test_data = np.array(X_test_data)\n",
    "y_test_data = np.array(y_test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b941d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preprocessing\n",
    "\n",
    "# Reshape the input data to 2-dimensional shape\n",
    "X_train_flat = X_train_data.reshape((X_train_data.shape[0], -1))\n",
    "X_val_flat = X_val_data.reshape((X_val_data.shape[0], -1))\n",
    "X_test_flat = X_test_data.reshape((X_test_data.shape[0], -1))\n",
    "\n",
    "# Handle missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_flat = imputer.fit_transform(X_train_flat)\n",
    "X_val_flat = imputer.transform(X_val_flat)\n",
    "X_test_flat = imputer.transform(X_test_flat)\n",
    "\n",
    "# Reshape the data back to the original shape\n",
    "X_train = X_train_flat.reshape((X_train_data.shape[0], window_size, -1))\n",
    "X_val = X_val_flat.reshape((X_val_data.shape[0], window_size, -1))\n",
    "X_test = X_test_flat.reshape((X_test_data.shape[0], window_size, -1))\n",
    "\n",
    "# Reshape the data for StandardScaler\n",
    "X_train_scaled = X_train.reshape((X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "X_val_scaled = X_val.reshape((X_val.shape[0] * X_val.shape[1], X_val.shape[2]))\n",
    "X_test_scaled = X_test.reshape((X_test.shape[0] * X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Apply StandardScaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "X_val_scaled = scaler.transform(X_val_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_scaled)\n",
    "\n",
    "# Reshape X_train_scaled, X_val_scaled, and X_test_scaled back to the original shape\n",
    "X_train_scaled = X_train_scaled.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_val_scaled = X_val_scaled.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2]))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de7a0ab2",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70248179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(64, activation='relu', input_shape=(window_size, X_train_scaled.shape[2])))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_data,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_scaled, y_val_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305f042e",
   "metadata": {},
   "source": [
    "# Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62af856b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# Convert the predicted probabilities to binary predictions\n",
    "y_pred_binary = np.where(y_pred_test > 0.5, 1, 0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate recall (sensitivity)\n",
    "recall = recall_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate specificity\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_data, y_pred_binary).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_data, y_pred_test)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(y_test_data, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall (Sensitivity):\", recall)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41076265",
   "metadata": {},
   "source": [
    "# Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bb3300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the variable names from the original dataset\n",
    "column_names = data.columns.tolist()\n",
    "\n",
    "# Visualize true values for each variable on the test set\n",
    "fig, axs = plt.subplots(len(column_names), figsize=(12, 6*len(column_names)))\n",
    "\n",
    "for i, column in enumerate(column_names):\n",
    "    actual_values_test = data_filtered[column].values[window_size:]\n",
    "    axs[i].plot(actual_values_test, label='Actual')\n",
    "    axs[i].set_xlabel('Time')\n",
    "    axs[i].set_ylabel('Value')\n",
    "    axs[i].set_title('Test Set: Variable {}'.format(column))\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0e8f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true values for the 'isSquealing' channel\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_data, label='Actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test Set: isSquealing (Actual)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize predicted values for the 'isSquealing' channel on the test set\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_pred_test, label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test Set: isSquealing (Predicted)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70b43f2",
   "metadata": {},
   "source": [
    "## Predicting isSquealing without filtered Trigger2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7536c488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.stats import zscore\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix, matthews_corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31abddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the directory path containing the CSV files\n",
    "directory = \"Two_Dataset_Selected_Columns_Int\"\n",
    "\n",
    "# Select the specific file for testing\n",
    "file_to_test = \"essai99_derivativeDataIA_combined.csv\"\n",
    "\n",
    "# Lists to store data\n",
    "X_train_data = []\n",
    "y_train_data = []\n",
    "X_val_data = []\n",
    "y_val_data = []\n",
    "X_test_data = []\n",
    "y_test_data = []\n",
    "\n",
    "# Split the files into training, validation, and testing sets\n",
    "file_list = os.listdir(directory)\n",
    "train_files, val_files, test_files = np.split(file_list, [int(len(file_list) * 0.7), int(len(file_list) * 0.85)])\n",
    "\n",
    "# Remove the test file from the test_files list\n",
    "test_files = [file for file in test_files if file != file_to_test]\n",
    "\n",
    "# Iterate over files in the specified directory\n",
    "for file in file_list:\n",
    "    if file.endswith(\".csv\"):\n",
    "        filepath = os.path.join(directory, file)\n",
    "        data = pd.read_csv(filepath)\n",
    "\n",
    "#         # Filter out instances where Trigger2 is not close to the most occurred value\n",
    "#         trigger2_mode = data['Trigger2'].mode()[0]\n",
    "#         close_threshold = 0.01  # Adjust this threshold as per your requirement\n",
    "#         data_filtered = data[np.abs(data['Trigger2'] - trigger2_mode) < close_threshold]\n",
    "        data_filtered = data\n",
    "        # Find the index of the 'isSquealing' column\n",
    "        isSquealing_index = list(data.columns).index('isSquealing')\n",
    "\n",
    "        # Split the data into input sequences (X) and output sequences (y)\n",
    "        window_size = 6\n",
    "        X = []\n",
    "        y = []\n",
    "        for i in range(len(data_filtered) - window_size):\n",
    "            X.append(data_filtered.iloc[i:i + window_size, :-1].values)\n",
    "            y.append(data_filtered.iloc[i + window_size, isSquealing_index])\n",
    "\n",
    "        if file in train_files:\n",
    "            X_train_data.extend(X)\n",
    "            y_train_data.extend(y)\n",
    "        elif file in val_files:\n",
    "            X_val_data.extend(X)\n",
    "            y_val_data.extend(y)\n",
    "        elif file == file_to_test:\n",
    "            X_test_data.extend(X)\n",
    "            y_test_data.extend(y)\n",
    "\n",
    "# Convert the lists to arrays\n",
    "X_train_data = np.array(X_train_data)\n",
    "y_train_data = np.array(y_train_data)\n",
    "X_val_data = np.array(X_val_data)\n",
    "y_val_data = np.array(y_val_data)\n",
    "X_test_data = np.array(X_test_data)\n",
    "y_test_data = np.array(y_test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f830dd8",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94e21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the input data to 2-dimensional shape\n",
    "X_train_flat = X_train_data.reshape((X_train_data.shape[0], -1))\n",
    "X_val_flat = X_val_data.reshape((X_val_data.shape[0], -1))\n",
    "X_test_flat = X_test_data.reshape((X_test_data.shape[0], -1))\n",
    "\n",
    "# Handle missing values using SimpleImputer\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_train_flat = imputer.fit_transform(X_train_flat)\n",
    "X_val_flat = imputer.transform(X_val_flat)\n",
    "X_test_flat = imputer.transform(X_test_flat)\n",
    "\n",
    "# Reshape the data back to the original shape\n",
    "X_train = X_train_flat.reshape((X_train_data.shape[0], window_size, -1))\n",
    "X_val = X_val_flat.reshape((X_val_data.shape[0], window_size, -1))\n",
    "X_test = X_test_flat.reshape((X_test_data.shape[0], window_size, -1))\n",
    "\n",
    "# Reshape the data for StandardScaler\n",
    "X_train_scaled = X_train.reshape((X_train.shape[0] * X_train.shape[1], X_train.shape[2]))\n",
    "X_val_scaled = X_val.reshape((X_val.shape[0] * X_val.shape[1], X_val.shape[2]))\n",
    "X_test_scaled = X_test.reshape((X_test.shape[0] * X_test.shape[1], X_test.shape[2]))\n",
    "\n",
    "# Apply StandardScaler to normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train_scaled)\n",
    "X_val_scaled = scaler.transform(X_val_scaled)\n",
    "X_test_scaled = scaler.transform(X_test_scaled)\n",
    "\n",
    "# Reshape X_train_scaled, X_val_scaled, and X_test_scaled back to the original shape\n",
    "X_train_scaled = X_train_scaled.reshape((X_train.shape[0], X_train.shape[1], X_train.shape[2]))\n",
    "X_val_scaled = X_val_scaled.reshape((X_val.shape[0], X_val.shape[1], X_val.shape[2]))\n",
    "X_test_scaled = X_test_scaled.reshape((X_test.shape[0], X_test.shape[1], X_test.shape[2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd74151d",
   "metadata": {},
   "source": [
    "# LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d57c967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the model architecture\n",
    "model = keras.Sequential()\n",
    "model.add(layers.LSTM(64, activation='relu', input_shape=(window_size, X_train_scaled.shape[2])))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Dense(1))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train_data,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_data=(X_val_scaled, y_val_data)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62fc1822",
   "metadata": {},
   "source": [
    "# Predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f162b7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate predictions on the test set\n",
    "y_pred_test = model.predict(X_test_scaled)\n",
    "\n",
    "\n",
    "# Convert the predicted probabilities to binary predictions\n",
    "y_pred_binary = np.where(y_pred_test > 0.5, 1, 0)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate precision\n",
    "precision = precision_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate recall (sensitivity)\n",
    "recall = recall_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate specificity\n",
    "tn, fp, fn, tp = confusion_matrix(y_test_data, y_pred_binary).ravel()\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "# Calculate F1 score\n",
    "f1 = f1_score(y_test_data, y_pred_binary)\n",
    "\n",
    "# Calculate ROC AUC score\n",
    "roc_auc = roc_auc_score(y_test_data, y_pred_test)\n",
    "\n",
    "# Calculate Matthews Correlation Coefficient (MCC)\n",
    "mcc = matthews_corrcoef(y_test_data, y_pred_binary)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall (Sensitivity):\", recall)\n",
    "print(\"Specificity:\", specificity)\n",
    "print(\"F1 Score:\", f1)\n",
    "print(\"ROC AUC Score:\", roc_auc)\n",
    "print(\"Matthews Correlation Coefficient (MCC):\", mcc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "503688d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the variable names from the original dataset\n",
    "column_names = data.columns.tolist()\n",
    "\n",
    "# Visualize true values for each variable on the test set\n",
    "fig, axs = plt.subplots(len(column_names), figsize=(12, 6*len(column_names)))\n",
    "\n",
    "for i, column in enumerate(column_names):\n",
    "    actual_values_test = data_filtered[column].values[window_size:]\n",
    "    axs[i].plot(actual_values_test, label='Actual')\n",
    "    axs[i].set_xlabel('Time')\n",
    "    axs[i].set_ylabel('Value')\n",
    "    axs[i].set_title('Test Set: Variable {}'.format(column))\n",
    "    axs[i].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def0581",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the true values for the 'isSquealing' channel\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_test_data, label='Actual')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test Set: isSquealing (Actual)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Visualize predicted values for the 'isSquealing' channel on the test set\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(y_pred_test, label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Value')\n",
    "plt.title('Test Set: isSquealing (Predicted)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
